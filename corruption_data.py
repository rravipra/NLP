# -*- coding: utf-8 -*-
"""corruption_data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17E1Re_TcEgTlQwSxh8AoFEcGjKbolrJ1
"""

import random
import nltk
from nltk.corpus import wordnet
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag

original_sentences = []
file_name = "" # the path to your train file

with open(file_name, 'r', encoding = "ISO-8859-1") as file:
    for line in file:
        sentence1, sentence2 = line.strip().split('\t')
        original_sentences.append(sentence1)

nltk.download('wordnet')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')


def get_random_word(start_char, length):
    words = [word for word in wordnet.words() if word[0] == start_char and len(word) == length]
    return random.choice(words) if words else None

def corrupt_sentence(sentence, corruption_prob=1):
    words = word_tokenize(sentence)
    tagged_words = pos_tag(words)
    
    corrupted_words = []
    
    remove_word = False # for removing at most 1 word:
    
    for word, tag in tagged_words:
        if random.random() < corruption_prob and not remove_word:
            if random.random() < 0.33:
                word = word[:-1] + random.choice('abcdefghijklmnopqrstuvwxyz')
            else:
                new_word = get_random_word(word[0], len(word))
                if new_word:
                    word = new_word
            remove_word = True # the word has been removed
        corrupted_words.append(word)
    
    # Remove a random word if no word has been removed yet and just remove 1 word at max.
    if not remove_word and len(corrupted_words) > 1:
        index = random.randint(0, len(corrupted_words) - 1)
        corrupted_words.pop(index)
    
    corrupted_sentence = ' '.join(corrupted_words)
    
    # Ensure the corrupted sentence is different from the original sentence in any case and not just by a space:
    while corrupted_sentence.lower() == sentence.lower() or corrupted_sentence == sentence or corrupted_sentence.replace(" ", "") == sentence.replace(" ", ""):
        corrupted_sentence = corrupt_sentence(sentence, corruption_prob)
    
    return corrupted_sentence

corrupted_sentences = []
for sentence in original_sentences:
  corrupted_sentences.append(corrupt_sentence(sentence))

with open("part2.txt", "w") as file:
  for original, corrupted in zip(original_sentences, corrupted_sentences):
    file.write(original + "\t" + corrupted + "\n")