# -*- coding: utf-8 -*-
"""Translators.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eeJIE419AqsRjly0jT6ZELVNQQGjyuA7

# Sequence to Sequence translator using an LSTM model.
"""

# Author: Rthvik Raviprakash
# email: rravipra@uw.edu
# Please look at the bottom of this code file for references used:

# import the required libraries
import string 
import re
import numpy as np 
from numpy import array, argmax, random, take 
import pandas as pd 
from keras.models import Sequential 
from keras.layers import Dense, LSTM, Embedding, RepeatVector
from keras.preprocessing.text import Tokenizer
from keras.callbacks import ModelCheckpoint 
from keras.utils import pad_sequences
from keras.models import load_model 
from keras import optimizers 
import matplotlib.pyplot as plt 
pd.set_option('display.max_colwidth', 200)

import pandas as pd
import re

from google.colab import drive
drive.mount('/content/drive')

# Download the file
data = pd.read_csv('/content/drive/MyDrive/NLP class assignment files/Train.tsv',sep='\t')
li_0 = data.columns
data

data.columns = ["chinese", "english", "spanish", "hindi", "japanese", "norwegian"]
data.loc[-1] = li_0
data.index = data.index + 1
df = data.sort_index()
df.head(15)

list(df['chinese'].to_list()[50])

def space_left(li_test):
  for tup in li_test:
    tup[0] = ' '.join(tup[0])
  return li_test

def space_right(li_tl):
  for tup in li_tl:
    tup[1] = ' '.join(tup[1])
  return li_tl

def space(li_both):
  for tup in li_both:
    tup[0] = ' '.join(tup[0])
    tup[1] = ' '.join(tup[1])
  return li_both

# Chinese to all
chn_eng = space_left(df.reset_index()[["chinese", "english"]].values.tolist())
chn_spn = space_left(df.reset_index()[["chinese", "spanish"]].values.tolist())
chn_hin = space_left(df.reset_index()[["chinese", "hindi"]].values.tolist())
chn_jap = space(df.reset_index()[["chinese", "japanese"]].values.tolist())
chn_nor = space_left(df.reset_index()[["chinese", "norwegian"]].values.tolist())

# English to all
eng_chn = space_right(df.reset_index()[["english", "chinese"]].values.tolist())
eng_spn = df.reset_index()[["english", "spanish"]].values.tolist()
eng_hin = df.reset_index()[["english", "hindi"]].values.tolist()
eng_jap = space_right(df.reset_index()[["english", "japanese"]].values.tolist())
eng_nor = df.reset_index()[["english", "norwegian"]].values.tolist()

# Spanish to all
spn_chn = space_right(df.reset_index()[["spanish", "chinese"]].values.tolist())
spn_eng = df.reset_index()[["spanish", "english"]].values.tolist()
spn_hin = df.reset_index()[["spanish", "hindi"]].values.tolist()
spn_jap = space_right(df.reset_index()[["spanish", "japanese"]].values.tolist())
spn_nor = df.reset_index()[["spanish", "norwegian"]].values.tolist()

# Hindi to all
hin_chn = space_right(df.reset_index()[["hindi", "chinese"]].values.tolist())
hin_eng = df.reset_index()[["hindi", "english"]].values.tolist()
hin_spn = df.reset_index()[["hindi", "spanish"]].values.tolist()
hin_jap = space_right(df.reset_index()[["hindi", "japanese"]].values.tolist())
hin_nor = df.reset_index()[["hindi", "norwegian"]].values.tolist()

# Japanese to all
jap_chn = space(df.reset_index()[["japanese", "chinese"]].values.tolist())
jap_eng = space_left(df.reset_index()[["japanese", "english"]].values.tolist())
jap_spn = space_left(df.reset_index()[["japanese", "spanish"]].values.tolist())
jap_hin = space_left(df.reset_index()[["japanese", "hindi"]].values.tolist())
jap_nor = space_left(df.reset_index()[["japanese", "norwegian"]].values.tolist())

# Norwegian to all
nor_chn = space_right(df.reset_index()[["norwegian", "chinese"]].values.tolist())
nor_eng = df.reset_index()[["norwegian", "english"]].values.tolist()
nor_spn = df.reset_index()[["norwegian", "spanish"]].values.tolist()
nor_hin = df.reset_index()[["norwegian", "hindi"]].values.tolist()
nor_jap = space_right(df.reset_index()[["norwegian", "japanese"]].values.tolist())

from sklearn.model_selection import train_test_split 

def tokenization(lines): 
      tokenizer = Tokenizer() 
      tokenizer.fit_on_texts(lines) 
      return tokenizer

def build_model(in_vocab,out_vocab, in_timesteps,out_timesteps,n):   
      model = Sequential() 
      model.add(Embedding(in_vocab, n, input_length=in_timesteps,   
      mask_zero=True)) 
      model.add(LSTM(n)) 
      model.add(RepeatVector(out_timesteps)) 
      model.add(LSTM(n, return_sequences=True))  
      model.add(Dense(out_vocab, activation='softmax')) 
      return model

def encode_sequences(tokenizer, length, lines):          
         # integer encode sequences          
         seq = tokenizer.texts_to_sequences(lines)          
         # pad sequences with 0 values          
         seq = pad_sequences(seq, maxlen=length, padding='post')           
         return seq
         
def get_word(n, tokenizer):  
      for word, index in tokenizer.word_index.items():                       
          if index == n: 
              return word 
      return None

# A model for all the langauge pairs but have to run them individually
def model_gen(file_n, arr_tup, ep):
  arr_tup = np.array(arr_tup)
  arr_tup[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in arr_tup[:,0]] 
  arr_tup[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in arr_tup[:,1]] 

  # convert text to lowercase 
  for i in range(len(arr_tup)): 
      arr_tup[i,0] = arr_tup[i,0].lower() 
      arr_tup[i,1] = arr_tup[i,1].lower()

  # empty lists 
  lang_1 = [] 
  lang_2 = [] 

  # populate the lists with sentence lengths 
  for i in arr_tup[:,0]: 
        lang_1.append(len(i.split())) 

  for i in arr_tup[:,1]: 
        lang_2.append(len(i.split())) 

  len_all = max(max(lang_1), max(lang_2))

  # prepare langauge 1 tokenizer 
  lang1_tokenizer = tokenization(arr_tup[:, 0]) 
  lang1_vocab_size = len(lang1_tokenizer.word_index) + 1 
  lang1_length = len_all

  # prepare language 2 tokenizer
  lang2_tokenizer = tokenization(arr_tup[:, 1]) 
  lang2_vocab_size = len(lang2_tokenizer.word_index) + 1 
  lang2_length = len_all

  # split data into train and test set 
  train,test= train_test_split(arr_tup,test_size=0.2,random_state= 12)

  # prepare training data 
  trainX = encode_sequences(lang2_tokenizer, lang2_length, train[:, 1]) 
  trainY = encode_sequences(lang1_tokenizer, lang1_length, train[:, 0]) 

  model = build_model(lang2_vocab_size, lang1_vocab_size, lang2_length, lang1_length, 512)

  rms = optimizers.RMSprop(lr=0.001) 
  model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')

  filename = file_n

  # set checkpoint
  checkpoint = ModelCheckpoint(filename, monitor='val_loss',  
                              verbose=1, save_best_only=True, 
                              mode='min') 

  # train model 
  history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1), 
                      epochs=ep, batch_size=512, validation_split = 0.2, 
                      callbacks=[checkpoint], verbose=1)

  return file_n, history, len_all

def pred(sent, file_na, arr, len_al):
    model = load_model(file_na)
    arr = np.array(arr)
    test = encode_sequences(tokenization(arr[:, 1]), len_al, np.array(sent))
    y_probs = model.predict(test)
    preds = np.argmax(y_probs, axis = -1)
    preds_text = []
    print("pr", preds) 
    for i in preds:        
      temp = []        
      for j in range(len(i)):             
            t = get_word(i[j], tokenization(arr[:, 0]))            
            if j > 0:                 
                if (t == get_word(i[j-1],tokenization(arr[:, 0])))or(t == None):                       
                    temp.append('')                 
                else:                      
                    temp.append(t)             
            else:                    
                if(t == None):                                   
                    temp.append('')                    
                else:                           
                    temp.append(t)        
      preds_text.append(' '.join(temp))

    return preds_text

# Model Training (The output below is training for Japanese to Norwegian)
len_a = model_gen('/content/drive/MyDrive/Translators/jap_nor_translate', nor_jap, 30)[2]

with open('/content/drive/MyDrive/Translators/lengths','w') as writefile:
  writefile.write(str(len_a) + " is the length for nor/eng" + "\n")

len_a

preds = pred(li_s, '/content/drive/MyDrive/Translators/eng_chn_translate', chn_eng, len_a)

# Made final output dataset
data_out = pd.read_csv('/content/drive/MyDrive/NLP class assignment files/Test.tsv',sep='\t')
li_out = data_out.columns
data_out.columns = ["chinese", "english", "spanish", "hindi", "japanese", "norwegian"]
data_out.loc[-1] = li_out
data_out.index = data_out.index + 1
df_out = data_out.sort_index()
df_out.head(15)

df_out.iloc[0]['english'] = float('nan')
df_out.iloc[0]['hindi'] = float('nan')
df_out.iloc[0]['japanese'] = float('nan')
df_out.iloc[0]['norwegian'] = float('nan')

di = {0: 'chinese', 1: 'english', 2: 'spanish', 3: 'hindi', 4: 'japanese', 5: 'norwegian'}
di_tr = {(0,1): ('/content/drive/MyDrive/chn_eng_translate', eng_chn, 11), (0,2): ('/content/drive/MyDrive/chn_spn_translate', spn_chn, 11), (0,3): ('/content/drive/MyDrive/chn_hin_translate', hin_chn, 11), (0,4): ('/content/drive/MyDrive/chn_jap_translate', jap_chn, 11), (0,5): ('/content/drive/MyDrive/chn_nor_translate', nor_chn, 11), 
         (1,0): ('/content/drive/MyDrive/eng_chn_translate', chn_eng, 11), (1,2): ('/content/drive/MyDrive/eng_spn_translate', spn_eng, 9), (1,3): ('/content/drive/MyDrive/eng_hin_translate', hin_eng, 7), (1,4): ('/content/drive/MyDrive/eng_jap_translate', jap_eng, 11), (1,5): ('/content/drive/MyDrive/eng_nor_translate', nor_eng, 9),
         (2,0): ('/content/drive/MyDrive/spn_chn_translate', chn_spn, 11), (2,1): ('/content/drive/MyDrive/spn_eng_translate', eng_spn, 9), (2,3): ('/content/drive/MyDrive/spn_hin_translate', hin_spn, 9), (2,4): ('/content/drive/MyDrive/spn_jap_translate', jap_spn, 11), (2,5): ('/content/drive/MyDrive/spn_nor_translate', nor_spn, 9), 
         (3,0): ('/content/drive/MyDrive/hin_chn_translate', chn_hin, 11), (3,1): ('/content/drive/MyDrive/hin_eng_translate', eng_hin, 7), (3,2): ('/content/drive/MyDrive/hin_spn_translate', spn_hin, 9), (3,4): ('/content/drive/MyDrive/Translators/hin_jap_translate', jap_hin, 11), (3,5): ('/content/drive/MyDrive/hin_nor_translate', nor_hin, 9),
         (4,0): ('/content/drive/MyDrive/Translators/jap_chn_translate', chn_jap, 11), (4,1): ('/content/drive/MyDrive/jap_eng_translate', eng_jap, 11), (4,2): ('/content/drive/MyDrive/jap_spn_translate', spn_jap, 11), (4,3): ('/content/drive/MyDrive/Translators/jap_hin_translate', hin_jap, 11), (4,5): ('/content/drive/MyDrive/Translators/jap_nor_translate', nor_jap, 11), 
         (5,0): ('/content/drive/MyDrive/nor_chn_translate', chn_nor, 11), (5,1): ('/content/drive/MyDrive/nor_eng_translate', eng_nor, 9), (5,2): ('/content/drive/MyDrive/nor_spn_translate', spn_nor, 9), (5,3): ('/content/drive/MyDrive/nor_hin_translate', hin_nor, 9), (5,4): ('/content/drive/MyDrive/Translators/nor_jap_translate', jap_nor, 11)}

res = []
res_dict = {'chinese': [], 'english': [], 'spanish': [], 'hindi':[], 'japanese':[], 'norwegian':[]}

for i in range(len(df_out.head(500))): # Here I have used 500 but we would want to run for the whole dataset. I only do 500 for not having much computational power.
  li = df_out.iloc[i].to_list()
  idx = 0
  idx1 = 0
  s1 = ""
  for j in range(len(li)):
    if li[j] == '?':
      idx_1 = j
    if type(li[j]) == str and li[j] != '?':
      idx_0 = j
      s1 = li[j]
  
  tri = di_tr[(idx_0, idx_1)]

  if idx_1 == 0:
    if idx_0 == 4:
      s1 = ' '.join(s1)
      predict = pred([s1], tri[0], tri[1], tri[2])[0]
      li[idx_1] = re.sub(' +', ' ', predict).replace(" ", '')
    else:
      predict_1 = pred([s1], tri[0], tri[1], tri[2])[0]
      li[idx_1] = re.sub(' +', ' ', predict_1).replace(" ", '')

  elif idx_1 == 4:
    if idx_0 == 0:
      s1 = ' '.join(s1)
      predict = pred([s1], tri[0], tri[1], tri[2])[0]
      li[idx_1] = re.sub(' +', ' ', predict).replace(" ", '')
    else:
      predict_1 = pred([s1], tri[0], tri[1], tri[2])[0]
      li[idx_1] = re.sub(' +', ' ', predict_1).replace(" ", '')
  
  else:
    if idx_0 == 0:
      s1 = ' '.join(s1)
      predict = pred([s1], tri[0], tri[1], tri[2])[0]
      li[idx_1] = re.sub(' +', ' ', predict)
    elif idx_0 == 4:
      s1 = ' '.join(s1)
      predict = pred([s1], tri[0], tri[1], tri[2])[0]
      li[idx_1] = re.sub(' +', ' ', predict)
    else:
      predict = pred([s1], tri[0], tri[1], tri[2])[0]
      li[idx_1] = re.sub(' +', ' ', predict)
  
    res_dict['chinese'].append(li[0])
    res_dict['english'].append(li[1])
    res_dict['spanish'].append(li[2])
    res_dict['hindi'].append(li[3])
    res_dict['japanese'].append(li[4])
    res_dict['norwegian'].append(li[5])

res_dict # Example of a result dictionary for the first two rows of the data

from google.colab import files

df_res = pd.DataFrame(res_dict)
df_res.to_csv('Test-output.tsv', sep="\t")
files.download('Test-output.tsv')

# For some of the code I have taken it from a Medium article: https://medium.com/analytics-vidhya/a-must-read-nlp-tutorial-on-neural-machine-translation-the-technique-powering-google-translate-c5c8d97d7587